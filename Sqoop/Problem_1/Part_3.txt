**************************************************************************************************************************
Part3: Using Spark Scala load data at /user/cloudera/problem1/orders and /user/cloudera/problem1/orders-items items as dataframes. 
or 
uploaded table records into hdfs path
**************************************************************************************************************************
scala> import com.databricks.spark.avro._;
scala> val ordersDF = sqlContext.read.avro("/user/cloudera/problem1/orders")
ordersDF: org.apache.spark.sql.DataFrame = [order_id: int, order_date: bigint, order_customer_id: int, order_status: string]

scala> val orderItemsDF = sqlContext.read.avro("/user/cloudera/problem1/order-items")
orderItemsDF: org.apache.spark.sql.DataFrame = [order_item_id: int, order_item_order_id: int, order_item_product_id: int, order_item_quantity: int, order_item_subtotal: float, order_item_product_price: float]
