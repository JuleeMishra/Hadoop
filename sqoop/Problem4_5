In this problem, we will focus on conversion between different file formats using spark or hive. This is a very import examination topic. I recommend that you master the data file conversion techniques and understand the limitations. You should have an alternate method of accomplishing a solution to the problem in case your primary method fails for any unknown reason. For example, if saving the result as a text file with snappy compression fails while using spark then you should be able to accomplish the same thing using Hive. In this blog\video I am going to walk you through some scenarios that cover alternative ways of dealing with same problem.    

------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------
PART: 4.5
Transform/Convert data-files at /user/cloudera/problem5/parquet-snappy-compress and store the converted file at the following locations and file formats
** save the data to hdfs using no compression as parquet file at /user/cloudera/problem5/parquet-no-compress
** save the data to hdfs using snappy compression as avro file at /user/cloudera/problem5/avro-snappy
------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------
** save the data to hdfs without compression as parquet file at /user/cloudera/problem5/parquet-no-compress
read the compressed avro datafile and store it as uncomressed paraquet file. (NOTE: there is no difference between reading a compressed and uncompressed datafile in spark)
	scala> sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
	>>>read the compressed parquet file
	scala> val parquetDataFile = sqlContext.read.parquet("/user/cloudera/problem5/parquet-snappy-compress")
	parquetDataFile: org.apache.spark.sql.DataFrame = [order_id: int, order_date: bigint, order_customer_id: int, order_status: string]
	>>>after reading save the read datafile as uncompressed paraquet file.
	scala> parquetDataFile.write.parquet("/user/cloudera/problem5/parquet-no-compress")
	18/04/02 17:44:22 WARN parquet.CorruptStatistics: Ignoring statistics because created_by is null or empty! See PARQUET-251 and PARQUET-297


>>>>>save the data to hdfs using snappy compression as avro file at /user/cloudera/problem5/avro-snappy
	scala> sqlContext.setConf("spark.sql.avro.compression.codec","snappy")
	scala> parquetDataFile.write.avro("/user/cloudera/problem5/avro-snappy")

