In this problem, we will focus on conversion between different file formats using spark or hive. This is a very import examination topic. I recommend that you master the data file conversion techniques and understand the limitations. You should have an alternate method of accomplishing a solution to the problem in case your primary method fails for any unknown reason. For example, if saving the result as a text file with snappy compression fails while using spark then you should be able to accomplish the same thing using Hive. In this blog\video I am going to walk you through some scenarios that cover alternative ways of dealing with same problem.    
------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------
PART: 4.7
** Transform/Convert data-files at  /user/cloudera/problem5/json-gzip and store the converted file at the following locations and file formats
** save the data to as comma separated text using gzip compression at   /user/cloudera/problem5/csv-gzip
------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------
** save the data to as comma separated text using gzip compression at   /user/cloudera/problem5/csv-gzip
transform the compressed json file to compressed textfile
	a)read the compressed json file
	scala> var jsonData = sqlContext.read.json("/user/cloudera/problem5/json-gzip");
	b)convert the jsonData into a comma separated to convert it in text format then compress it as gzip
	jsonData.map(x=>x(0)+","+x(1)+","+x(2)+","+x(3)).saveAsTextFile("/user/cloudera/problem5/csv-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])


