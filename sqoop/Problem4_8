In this problem, we will focus on conversion between different file formats using spark or hive. This is a very import examination topic. I recommend that you master the data file conversion techniques and understand the limitations. You should have an alternate method of accomplishing a solution to the problem in case your primary method fails for any unknown reason. For example, if saving the result as a text file with snappy compression fails while using spark then you should be able to accomplish the same thing using Hive. In this blog\video I am going to walk you through some scenarios that cover alternative ways of dealing with same problem.    


------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------
PART: 4.8
Using spark access data at /user/cloudera/problem5/sequence and stored it back to hdfs using no compression as ORC file to HDFS to destination /user/cloudera/problem5/orc 
------------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------------

NOTE: to read sequence file you need to understand class of key and value.
reading sequence file through sc context takes 3 parameters.
sc.sequenceFile("path of the sequenceFile", class of Key, Class of Value)
>>understanding class of key
[cloudera@quickstart ~]$ hadoop fs -copyToLocal /user/cloudera/problem5/sequence/part-m-00000
>>get 1st 300 caracter of sequence file using cut command
[cloudera@quickstart ~]$ cut -c-300 part-m-00000
here both key and value are of text class
so,
scala> val seqDataFile = sc.sequenceFile("/user/cloudera/problem5/sequence", classOf[org.apache.hadoop.io.Text], classOf[org.apache.hadoop.io.Tesxt])


 
